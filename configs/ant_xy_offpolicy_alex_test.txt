# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

### TRAINING HYPERPARAMETERS -------------------
--run_train=1

# metadata flags
--save_model=dads
--save_freq=50
--record_freq=100
--vid_name=latent

# optimization hyperparmaters
--replay_buffer_capacity=10000

# (set clear_buffer_every_iter=1 for on-policy optimization)
--clear_buffer_every_iter=0
--initial_collect_steps=2000
--collect_steps=500
--num_epochs=12000

# latent dynamics optimization hyperparameters
--latent_dyn_train_steps=8
--latent_dynamics_lr=3e-4
--latent_dyn_batch_size=256

# agent hyperparameters
--agent_gamma=0.99
--agent_lr=3e-4
--agent_entropy=0.1
--agent_train_steps=64
--agent_batch_size=256

# (optional, do not change for on-policy) relabelling or off-policy corrections
--latent_dynamics_relabel_type=importance_sampling
--num_samples_for_relabelling=1
--is_clip_eps=10.

# (optional) latents can be resampled within the episodes, relative to max_env_steps
--min_steps_before_resample=2000
--resample_prob=0.02

# (optional) configure latent dynamics training samples to be only from the current policy
--train_latent_dynamics_on_policy=0

### SHARED HYPERPARAMETERS ---------------------
--environment=point_mass
--max_env_steps=100
# this decide processed dimension
--reduced_observation=2

# define the type of latents being learnt
--latent_type=cont_uniform
--skill_latent_type=cont_uniform
--style_latent_type=cont_uniform
--random_latents=100
--num_evals=4

--num_latents=2
--num_skills=1
--num_styles=1

--prediction_after_n_steps = 1

# number of steps for sampling
# TODO: The steps at the end of the trajectory may have less probability to be sampled.
--num_sampling_steps=20

# this parameter only works when the latent_type is cont_uniform. it will make the evaluation evenly sample the uniform distribution 
--cont_uniform_method=evenly


# (optional) policy, critic and latent dynamics
--hidden_layer_size=512

# (optional) latent dynamics hyperparameters
--graph_type=default
--num_components=4
--fix_variance=1
--normalize_data=1

# (optional) clip sampled actions
--action_clipping=1.

# (optional) debugging
--debug=0

### EVALUATION HYPERPARAMETERS -----------------
--run_eval=0

# MPC hyperparameters
--planning_horizon=1
--primitive_horizon=10
--num_candidate_sequences=50
--refine_steps=10
--mppi_gamma=10
--prior_type=normal
--smoothing_beta=0.9
--top_primitives=5


### (optional) ENVIRONMENT SPECIFIC HYPERPARAMETERS --------
# DKitty hyperparameters
--expose_last_action=1
--expose_upright=1
--robot_noise_ratio=0.0
--root_noise_ratio=0.0
--upright_threshold=0.95
--scale_root_position=1
--randomize_hfield=0.0

# DKitty/DClaw
--observation_omission_size=0

# Cube Manipulation hyperparameters
--randomized_initial_distribution=1
--horizontal_wrist_constraint=0.3
--vertical_wrist_constraint=1.0
